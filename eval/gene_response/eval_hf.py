import json
from fastchat.model import get_conversation_template
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

LLAMA2_SYSPROMPT = "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."
MISTRAL_SYSPROMPT = "Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity."

_MODELS = {
    "llama2_chat_7b": ["meta-llama/Llama-2-7b-chat-hf", "llama-2", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}],
    "llama2_chat_13b": ["meta-llama/Llama-2-13b-chat-hf", "llama-2", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}], 
    "gemma_7b": ["google/gemma-7b-it", "gemma", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}],
    "gemma_2b": ["google/gemma-2b-it", "gemma", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}],
    "mistral_7b": ["mistralai/Mistral-7B-Instruct-v0.2", "mistral", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}],
    "mixtral_8x7b": ["mistralai/Mixtral-8x7B-Instruct-v0.1", "mistral", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}],
    "llama3_8b_it": ["meta-llama/Meta-Llama-3-8B-Instruct", "llama-3", {"low_cpu_mem_usage": True, "use_cache": False, "use_flash_attention_2": True}, {"use_fast": True}]
}

def parse_args():
    parser = argparse.ArgumentParser(
        description=
        "Generate responses")
    parser.add_argument(
        "--model",
        type=str,
        default='llama2_chat_7b',
        choices=['llama2_chat_7b', 'llama2_chat_13b', 'gemma_7b',
                'mistral_7b', "mpt_7b", "mixtral_8x7b", "gemma_2b", "llama3_8b_it"]
    )
    parser.add_argument(
        "--test_case_path",
        type=str,
        default='./test_cases.json',
        help="The path to the test cases"
    )
    parser.add_argument(
        "--generations_path",
        type=str,
        default='../generations.json',
        help="The path used for saving generations"
    )
    args = parser.parse_args()

    return args


def main():
    args = parse_args()
    model_name = args.model
    test_case_path = args.test_case_path
    generations_path = args.generations_path
    
    model = AutoModelForCausalLM.from_pretrained(
            _MODELS[model_name][0],
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="balanced",
            **_MODELS[model_name][2]
        )
    model = model.eval()
    model.requires_grad_(False)
    
    tokenizer = AutoTokenizer.from_pretrained(
            _MODELS[model_name][0],
            padding_side = 'left',
            trust_remote_code=True,
            **_MODELS[model_name][3]
        )
    if not tokenizer.pad_token or tokenizer.pad_token_id >= tokenizer.vocab_size:
        tokenizer.pad_token = tokenizer.eos_token

    conv_template = get_conversation_template(_MODELS[model_name][1])
    
    if "mistral" in model_name:
        conv_template.set_system_message(MISTRAL_SYSPROMPT)

    test_cases = json.load(open(test_case_path, 'r'))

    test_cases_formatted = {}
    for behavior in test_cases:
        test_cases_formatted[behavior] = []
        for tc in test_cases[behavior]:
            conv_template.messages = []
            conv_template.append_message(conv_template.roles[0], tc)
            conv_template.append_message(conv_template.roles[1], None)
            prompt = conv_template.get_prompt()
            test_cases_formatted[behavior].append(prompt)
    print(prompt)
    test_cases = test_cases_formatted
    
    gen_config = model.generation_config
    gen_config.do_sample = False
    gen_config.max_new_tokens = 256
    
    batch_size = 50
    generations = {}
    for i, behavior in enumerate(test_cases):
        print(f"({i+1}/{len(test_cases)}): \"{behavior}\"")
        generations[behavior] = []
        for j in range(0, len(test_cases[behavior]), batch_size):
            current_test_cases = test_cases[behavior][j:j+batch_size]
            input_ids = tokenizer(current_test_cases, padding=True, return_tensors="pt")
            input_ids['input_ids'] = input_ids['input_ids'].cuda()
            input_ids['attention_mask'] = input_ids['attention_mask'].cuda()
            print(input_ids['attention_mask'][-1])
            num_input_tokens = input_ids['input_ids'].shape[1]
            outputs = model.generate(input_ids['input_ids'], attention_mask=input_ids['attention_mask'].half(),
                                    generation_config=gen_config, pad_token_id=tokenizer.pad_token_id,
                                    top_p=1.0, temperature=0.0, do_sample=False)
            generation = tokenizer.batch_decode(outputs[:, num_input_tokens:], skip_special_tokens=True)
            generations[behavior].extend(generation)
        with open(generations_path, 'w') as f:
            json.dump(generations, f, sort_keys=True)

if __name__ == "__main__":
    main()
